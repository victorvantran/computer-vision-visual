{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c503cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import shutil\n",
    "import re \n",
    "import time\n",
    "\n",
    "RANDOM_SEED = 8675309"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba732ac",
   "metadata": {},
   "source": [
    "## Set up PASCAL_2010 image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c40de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = r\"../datasets\"\n",
    "PASCAL_2010_DIR = os.path.join(DATASETS_DIR, r\"PASCAL2010\")\n",
    "\n",
    "PASCAL_2010_ORIGINAL_IMAGES_DIR = r\"G:\\PASCAL_tinker\\datasets\\VOC2010_Context\\JPEGImages\" # RENAME TO WHERE THE JPEGImages Directory of Pascal2010 is located\n",
    "PASCAL_2010_ORIGINAL_LABELS_DIR = r\"G:\\PASCAL_tinker\\datasets\\VOC2010_Context\\SegmentationRawClass\" # RENAME TO WHERE THE SegmentationRawData Directory of Pascal2010 is located\n",
    "\n",
    "PASCAL_2010_IMAGES_DIR = os.path.join(PASCAL_2010_DIR, r\"images\")\n",
    "PASCAL_2010_LABELS_DIR = os.path.join(PASCAL_2010_DIR, r\"labels\")\n",
    "\n",
    "\n",
    "def set_up_pascal2010_dataset(\n",
    "    original_images_dir=PASCAL_2010_ORIGINAL_IMAGES_DIR, \n",
    "    original_labels_dir=PASCAL_2010_ORIGINAL_LABELS_DIR, \n",
    "    images_dir=PASCAL_2010_IMAGES_DIR,\n",
    "    labels_dir=PASCAL_2010_LABELS_DIR, \n",
    "    zfill=8):\n",
    "    for i,matrix_file in enumerate(os.listdir(original_labels_dir)):\n",
    "        deconstructed_file_name = re.split('\\_|\\.', matrix_file)\n",
    "        keep_image_file_name = deconstructed_file_name[0] + \"_\" + deconstructed_file_name[1] + \".\" + \"jpg\"\n",
    "        \n",
    "        mat_label = io.loadmat(os.path.join(original_labels_dir, matrix_file))['LabelMap']\n",
    "        new_label_file_name = \"PASCAL2010\" + \"_\" + str(i).zfill(zfill) + \".\" + \"npy\"\n",
    "        new_image_file_name = \"PASCAL2010\" + \"_\" + str(i).zfill(zfill) + \".\" + \"jpg\"\n",
    "        \n",
    "        np.save(os.path.join(labels_dir, new_label_file_name), mat_label)\n",
    "        shutil.copyfile(os.path.join(original_images_dir, keep_image_file_name), os.path.join(images_dir, new_image_file_name))\n",
    "\n",
    "        \n",
    "def run_set_up_pascal2010_dataset():\n",
    "    set_up_pascal2010_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff97cdb",
   "metadata": {},
   "source": [
    "# Set up ADE image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5384edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We already have the unprocessed .npy files processed from the .png segmentation files of ADE. Let's rename them to a compatible convetion '''\n",
    "\n",
    "DATASETS_DIR = r\"../datasets\"\n",
    "ADE_DIR = os.path.join(DATASETS_DIR, r\"ADE\")\n",
    "ADE_LABELS_DIR = os.path.join(ADE_DIR, r\"labels\")\n",
    "ADE_IMAGES_DIR = os.path.join(ADE_DIR, r\"images\")\n",
    "\n",
    "\n",
    "def rename_ade_labels(labels_dir=ADE_LABELS_DIR, zfill=8):\n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        deconstructed_file_name = re.split('\\_|\\.', label_file)\n",
    "        new_file_name = \"ADE\" + \"_\" + str(int(deconstructed_file_name[-2])).zfill(zfill) + \".\" + deconstructed_file_name[-1]\n",
    "        os.rename(os.path.join(labels_dir, label_file), os.path.join(labels_dir, new_file_name))\n",
    "            \n",
    "            \n",
    "def rename_ade_images(images_dir=ADE_IMAGES_DIR, zfill=8):\n",
    "    for image_file in os.listdir(images_dir):\n",
    "        deconstructed_file_name = re.split('\\_|\\.', image_file)\n",
    "        new_file_name = \"ADE\" + \"_\" + str(int(deconstructed_file_name[-2])).zfill(zfill) + \".\" + deconstructed_file_name[-1]\n",
    "        os.rename(os.path.join(images_dir, image_file), os.path.join(images_dir, new_file_name))\n",
    "        \n",
    "        \n",
    "def run_setup_ade_image_dataset():\n",
    "    rename_ade_images()\n",
    "    rename_ade_labels()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb81a13",
   "metadata": {},
   "source": [
    "## Create ADE Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8648edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.0029981136322021484\n"
     ]
    }
   ],
   "source": [
    "DATASETS_DIR = r\"../datasets\"\n",
    "ADE_DIR = os.path.join(DATASETS_DIR, r\"ADE\")\n",
    "ADE_LABELS_DIR = os.path.join(ADE_DIR, r\"labels\")\n",
    "ADE_DATAFRAMES_DIR = os.path.join(ADE_DIR, r\"dataframes\")\n",
    "ADE_MISC_DIR = os.path.join(ADE_DIR, \"miscellaneous\")     \n",
    "ADE_CLASS_FILE =  os.path.join(ADE_MISC_DIR, r\"class_list.txt\")\n",
    "\n",
    "ADE_CLASS_ID_INVALID = 0\n",
    "\n",
    "\n",
    "def get_ADE_classes(class_file):\n",
    "    classes_df = pd.read_csv(class_file, header=0, sep='\\t', lineterminator='\\n')\n",
    "    classes = np.asarray(classes_df['Name'])\n",
    "    for i in range(len(classes)):\n",
    "        classes[i] = re.split(',', classes[i])[0]\n",
    "        classes[i] = classes[i].rstrip()\n",
    "    return classes\n",
    "\n",
    "\n",
    "def get_num_instances(directory, extension):\n",
    "    # Return the number of instances in the dataset\n",
    "    instances = 0\n",
    "    for file in os.listdir(directory):\n",
    "        if re.split('\\_|\\.', file)[-1] == extension:\n",
    "            instances += 1\n",
    "    return instances\n",
    "\n",
    "\n",
    "def save_dataframe(df, dataset_dir, name, index=False):\n",
    "    dataframes_dir = os.path.join(dataset_dir, \"dataframes\")     \n",
    "    df.to_csv(os.path.join(dataframes_dir, name), index=index)\n",
    "\n",
    "    \n",
    "def load_dataframe(filename, dataset_dir, header=None, sep=',', index_col=0):\n",
    "    dataframes_dir = os.path.join(dataset_dir, \"dataframes\")     \n",
    "    return pd.read_csv(os.path.join(dataframes_dir, filename), header=header, sep=sep, index_col=index_col, lineterminator='\\n')\n",
    "\n",
    "\n",
    "def create_initial_df(dataset_dir, class_id_invalid):\n",
    "    ''' The initial ADE has features: id, width, height, scene, and invalid ratio '''\n",
    "    images_dir = os.path.join(dataset_dir, \"images\")     \n",
    "    labels_dir = os.path.join(dataset_dir, \"labels\")     \n",
    "    \n",
    "    total_num_images = get_num_instances(images_dir, \"jpg\")\n",
    "    total_num_labels = get_num_instances(labels_dir, \"npy\")\n",
    "    assert(total_num_images==total_num_labels)\n",
    "    total_num_instances = total_num_images\n",
    "    \n",
    "    id_col = np.zeros(total_num_instances, dtype=np.int64)   \n",
    "    width_col = np.zeros(total_num_instances, dtype=np.int64)   \n",
    "    height_col = np.zeros(total_num_instances, dtype=np.int64)\n",
    "    invalid_class_node_ratio_col = np.zeros(total_num_instances, dtype=np.float64)   \n",
    "\n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        label = np.load(os.path.join(labels_dir, label_file))\n",
    "        \n",
    "        instance_id = int(re.split('\\_|\\.', label_file)[-2])\n",
    "        width = label.shape[1]\n",
    "        height = label.shape[0]\n",
    "\n",
    "        id_col[instance_id] = instance_id\n",
    "        width_col[instance_id] = width\n",
    "        height_col[instance_id] = height\n",
    "        invalid_class_node_ratio_col[instance_id] = ((label == class_id_invalid).sum())/(width*height)\n",
    "    \n",
    "    # Create scene column using the text file given\n",
    "    miscellaneous_dir = os.path.join(dataset_dir, \"miscellaneous\")     \n",
    "    scene_categories_file = os.path.join(miscellaneous_dir, \"scene_categories.txt\")\n",
    "    if os.path.isfile(scene_categories_file) and os.access(scene_categories_file, os.R_OK):\n",
    "        scene_col = (pd.read_csv(scene_categories_file, header=None, sep=' ', lineterminator='\\n').iloc[:,1]).to_numpy() \n",
    "        assert(scene_col.shape[0] == total_num_instances)\n",
    "        initial_df = pd.DataFrame({\n",
    "            \"id\" : id_col,\n",
    "            \"width\" : width_col,\n",
    "            \"height\" : height_col,\n",
    "            \"scene\" : scene_col,\n",
    "            \"invalid\" : invalid_class_node_ratio_col,           \n",
    "            })\n",
    "    else:\n",
    "        initial_df = pd.DataFrame({\n",
    "            \"id\" : id_col,\n",
    "            \"width\" : width_col,\n",
    "            \"height\" : height_col,\n",
    "            \"invalid\" : invalid_class_node_ratio_col,           \n",
    "            })\n",
    "\n",
    "    #save_dataframe(df=initial_df, dataset_dir=dataset_dir, name=name)\n",
    "\n",
    "    return initial_df\n",
    "\n",
    "\n",
    "def create_class_proportion_df(dataset_dir, classes):\n",
    "    ''' Create pixel-ratio of each class to measure its apparency in an image. We can use pearson correlation to see which classes can be binned together '''\n",
    "    images_dir = os.path.join(dataset_dir, \"images\")     \n",
    "    labels_dir = os.path.join(dataset_dir, \"labels\")     \n",
    "\n",
    "    total_num_images = get_num_instances(images_dir, \"jpg\")\n",
    "    total_num_labels = get_num_instances(labels_dir, \"npy\")\n",
    "    assert(total_num_images==total_num_labels)\n",
    "    total_num_instances = total_num_images\n",
    "    \n",
    "    num_valid_classes=len(classes)\n",
    "    class_proportion = np.arange(1, num_valid_classes+1, 1, dtype=np.float64)   \n",
    "    class_proportion = np.tile(class_proportion,(total_num_instances,1))\n",
    "    \n",
    "    def proportion(class_id, label):\n",
    "        return ((label == class_id).sum())/label.size\n",
    "    \n",
    "    # https://stackoverflow.com/questions/4495882/numpy-vectorize-using-lists-as-arguments\n",
    "    # Use vectorization for speed-up\n",
    "    def curry_proportion(label):\n",
    "        def proportion_curried(class_id):\n",
    "            return proportion(class_id, label)  \n",
    "        return proportion_curried\n",
    "\n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        label_deconstructed = re.split('\\_|\\.', label_file)     \n",
    "        instance_id = int(label_deconstructed[-2])\n",
    "                \n",
    "        label = np.load(os.path.join(labels_dir, label_file)).flatten()\n",
    "        vector_proportion = np.vectorize(curry_proportion(label))\n",
    "        class_proportion[instance_id] = vector_proportion(class_proportion[instance_id])\n",
    "    \n",
    "    classes_df = pd.DataFrame(class_proportion, columns=classes)\n",
    "    \n",
    "    #save_dataframe(df=classes_df, dataset_dir=dataset_dir, name=name)\n",
    "    # Time elapsed: 1445.3204731941223\n",
    "    return classes_df\n",
    "    \n",
    "\n",
    "def create_master_df(dataset_dir, classes, class_id_invalid, name):\n",
    "    ''' Merge all the dataframes by column '''\n",
    "    initial_df=create_initial_df(dataset_dir=dataset_dir, class_id_invalid=class_id_invalid)\n",
    "    class_proportion_df=create_class_proportion_df(dataset_dir=dataset_dir, classes=classes)\n",
    "    \n",
    "    assert(initial_df.shape[0] == class_proportion_df.shape[0])\n",
    "    master_df = pd.concat([initial_df, class_proportion_df], axis=1)\n",
    "    \n",
    "    # Track the id of the instance\n",
    "    master_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Remove carriage return from the column names\n",
    "    master_df_cols = master_df.columns.values\n",
    "    for i in range(len(master_df_cols)):\n",
    "        master_df_cols[i] = master_df_cols[i].rstrip()\n",
    "    master_df.columns = master_df_cols\n",
    "    \n",
    "    # Add a column to indicate what dataset this instance belongs to\n",
    "    dataset_cols = [os.path.basename(dataset_dir) for _ in range(master_df.shape[0])]\n",
    "    master_df.insert(loc=2, column='dataset', value=dataset_cols)\n",
    "    \n",
    "    save_dataframe(df=master_df, dataset_dir=dataset_dir, name=name)\n",
    "\n",
    "    return master_df\n",
    "\n",
    "    \n",
    "def create_ade_dataframe():\n",
    "    start = time.time()\n",
    "    ade_classes = get_ADE_classes(class_file=ADE_CLASS_FILE)\n",
    "    create_master_df(dataset_dir=ADE_DIR,classes=ade_classes, class_id_invalid=ADE_CLASS_ID_INVALID, name=\"ADE_master.csv\")\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed: {}\".format(end-start)) #Time elapsed: 1928.032068014145"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd281fdd",
   "metadata": {},
   "source": [
    "## Create PASCAL Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713d8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.009996652603149414\n"
     ]
    }
   ],
   "source": [
    "DATASETS_DIR = r\"../datasets\"\n",
    "PASCAL2010_DIR = os.path.join(DATASETS_DIR, r\"PASCAL2010\")\n",
    "PASCAL2010_LABELS_DIR = os.path.join(PASCAL2010_DIR, r\"labels\")\n",
    "PASCAL2010_DATAFRAMES_DIR = os.path.join(PASCAL2010_DIR, r\"dataframes\")\n",
    "PASCAL2010_MISC_DIR = os.path.join(PASCAL2010_DIR, \"miscellaneous\")     \n",
    "PASCAL2010_CLASS_FILE =  os.path.join(PASCAL2010_MISC_DIR, r\"class_list.txt\")\n",
    "\n",
    "PASCAL2010_CLASS_ID_INVALID = 431\n",
    "\n",
    "def get_PASCAL2010_classes(class_file):\n",
    "    ''' Move unknown from 431 to 0 and slide the rest down? '''\n",
    "    classes_df = pd.read_csv(class_file, header=None, engine = 'python-fwf', lineterminator='\\r\\n')\n",
    "    classes = np.asarray(classes_df).flatten()\n",
    "    for i in range(len(classes)):\n",
    "        classes[i] = re.split(': ', classes[i])[1]\n",
    "        classes[i] = classes[i].rstrip()\n",
    "   \n",
    "    #classes = list(classes)\n",
    "    #classes.remove(\"unknown\")\n",
    "        \n",
    "    return np.asarray(classes)\n",
    "\n",
    "\n",
    "def rearrange_PASCAL2010_df(dataset_dir, name):\n",
    "    ''' Change unknown column to the invalid column '''\n",
    "    df = load_dataframe(filename=name, header=0, dataset_dir=dataset_dir, index_col=None)\n",
    "    \n",
    "    df[\"invalid\"] = df[\"unknown\"].values\n",
    "    df.drop([\"unknown\"], axis=1, inplace=True)\n",
    "    \n",
    "    save_dataframe(df=df, dataset_dir=dataset_dir, name=name)\n",
    "\n",
    "\n",
    "def create_pascal_dataframe():\n",
    "    start = time.time()\n",
    "    pascal2010_classes=get_PASCAL2010_classes(class_file=PASCAL2010_CLASS_FILE)\n",
    "    pascal2010_master_df=create_master_df(dataset_dir=PASCAL2010_DIR, classes=pascal2010_classes, class_id_invalid=PASCAL_CLASS_ID_INVALID, name=\"PASCAL2010_master.csv\")\n",
    "    rearrange_PASCAL2010_df(dataset_dir=PASCAL2010_DIR, name=\"PASCAL2010_master.csv\")\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed: {}\".format(end-start)) #Time elapsed: 1489.9916152954102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8b328",
   "metadata": {},
   "source": [
    "## Separate ADE dataset by indoor and outdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf239476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22210\n",
      "12755\n",
      "10237\n",
      "13055\n",
      "11453\n",
      "Indoor: 11821\n",
      "XIndoor: 368\n",
      "588        588\n",
      "944        944\n",
      "951        951\n",
      "960        960\n",
      "968        968\n",
      "         ...  \n",
      "21870    21870\n",
      "21956    21956\n",
      "22089    22089\n",
      "22121    22121\n",
      "22145    22145\n",
      "Name: id, Length: 368, dtype: int64\n",
      "9040\n",
      "4397\n",
      "Sidewalk: 3372\n",
      "Road no sidewalk: 1694\n",
      "1360\n",
      "Walkable grass: 2446\n",
      "[562, 563, 571, 572, 579, 582, 933, 934, 941, 945, 950, 969, 970, 973, 975, 976, 979, 980, 982, 983, 984, 986, 987, 988, 991, 992, 994, 996, 1002, 1003, 1004, 1005, 1007, 1010, 1011, 1012, 1014, 1015, 1018, 1021, 1024, 1025, 1029, 1032, 1033, 1034, 1036, 1038, 1080, 1084, 1085, 1086, 1090, 1313, 1315, 1323, 1324, 1369, 1376, 1403, 1411, 1414, 1417, 1422, 1424, 1428, 1429, 1432, 1438, 1458, 1466, 1481, 1482, 1483, 1505, 1507, 1510, 1511, 1516, 1520, 1527, 1528, 1530, 1535, 1545, 1546, 1547, 1588, 1589, 1621, 1853, 1854, 2037, 2039, 2040, 2071, 2081, 2082, 2083, 2087, 2091, 2093, 2096, 2107, 2111, 2116, 2258, 2317, 2352, 2354, 2355, 2357, 2360, 2362, 2363, 2430, 2431, 2432, 2434, 2435, 2436, 2437, 2438, 2439, 2441, 2443, 2450, 2451, 2452, 2453, 2454, 2458, 2464, 2471, 2472, 2506, 2510, 2522, 2523, 2524, 2526, 3019, 3020, 3023, 3036, 3037, 3039, 3041, 3042, 3050, 3052, 3055, 3118, 3144, 3151, 3152, 4232, 4234, 4285, 4289, 4321, 4325, 4327, 4330, 4346, 4351, 4353, 4354, 4362, 4364, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4428, 4470, 4471, 4476, 4493, 4530, 4532, 4541, 4545, 4546, 4547, 4549, 4554, 4557, 4566, 4570, 4585, 4587, 4589, 4592, 4607, 4608, 4611, 4623, 4630, 4640, 4641, 4643, 4693, 4707, 4710, 4712, 4714, 4724, 4741, 4743, 4744, 4745, 4751, 4753, 4759, 4771, 4781, 4791, 4804, 4819, 4828, 4837, 4847, 4850, 4851, 4883, 4885, 4890, 4899, 4914, 4921, 4926, 4934, 4935, 4937, 4938, 4940, 4941, 4942, 4944, 4945, 4946, 4965, 4968, 4971, 4976, 4980, 4982, 4983, 4984, 4985, 4987, 4988, 4989, 4991, 4992, 4993, 4995, 4999, 5000, 5004, 5005, 5007, 5010, 5011, 5022, 5121, 5123, 5126, 5136, 5208, 5216, 5219, 5221, 5223, 5224, 5225, 5227, 5228, 5230, 5231, 5233, 5235, 5236, 5238, 5239, 5240, 5244, 5245, 5247, 5250, 5251, 5252, 5253, 5255, 5257, 5259, 5260, 5263, 5267, 5268, 5271, 5273, 5276, 5311, 5313, 5316, 5320, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5363, 5364, 5365, 5367, 5368, 5414, 5415, 5418, 5437, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5455, 5456, 5459, 5460, 5565, 5569, 5571, 5572, 5575, 5576, 5585, 5586, 5588, 5589, 5591, 5593, 5695, 5699, 5700, 5701, 5702, 5703, 5704, 5805, 5811, 5849, 5853, 5870, 5883, 6154, 6166, 6188, 6191, 6192, 6193, 6195, 6197, 6198, 6200, 6206, 6221, 6222, 6233, 6241, 6248, 6252, 6363, 6364, 6365, 6367, 6369, 6370, 6372, 6376, 6377, 6378, 6382, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6393, 6394, 6396, 6397, 6398, 6399, 6400, 6401, 6420, 6422, 6428, 6436, 6440, 6443, 6445, 6472, 6475, 6494, 6505, 6517, 6568, 6572, 6575, 6577, 6587, 6593, 6680, 6712, 6713, 6723, 6725, 7262, 7275, 7276, 7278, 7280, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7297, 7298, 7299, 7300, 7302, 7303, 7304, 7342, 7343, 7417, 7421, 7426, 7430, 7432, 7454, 7457, 7461, 7462, 7464, 7467, 7469, 7470, 7472, 7479, 7481, 7483, 7484, 7485, 7486, 7487, 7488, 7500, 7501, 7528, 7599, 7603, 7642, 7660, 7662, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 7700, 7701, 7704, 7709, 7710, 7711, 7736, 7738, 7742, 7743, 7744, 7745, 7763, 7766, 7771, 7772, 7785, 7787, 7793, 7801, 7803, 7807, 7808, 7810, 7817, 7828, 7833, 7852, 7858, 7860, 7869, 7873, 7878, 7881, 7898, 7901, 7917, 7918, 7961, 7962, 7985, 7987, 7988, 7991, 8037, 8038, 8040, 8041, 8042, 8043, 8044, 8045, 8049, 8051, 8061, 8068, 8072, 8084, 8087, 8107, 8121, 8123, 8128, 8144, 8174, 8179, 8181, 8182, 8184, 8195, 8198, 8199, 8201, 8208, 8209, 8210, 8213, 8214, 8218, 8221, 8222, 8229, 8230, 8232, 8234, 8236, 8241, 8244, 8248, 8250, 8252, 8258, 8259, 8261, 8264, 8267, 8270, 8272, 8276, 8277, 8278, 8281, 8285, 8432, 8498, 8503, 8504, 8506, 8507, 8511, 8516, 8522, 8529, 8530, 8532, 8537, 8538, 8539, 8543, 8548, 8551, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8562, 8563, 8565, 8566, 8567, 8568, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8590, 8592, 8594, 8598, 8608, 8610, 8612, 8614, 8621, 8622, 8624, 8632, 8636, 8656, 8657, 8658, 8659, 8660, 8661, 8662, 8663, 8666, 8667, 8668, 8669, 8670, 8671, 8673, 8675, 8676, 8677, 8678, 8680, 8681, 8682, 8683, 8684, 8685, 8686, 8687, 8690, 8700, 8705, 8747, 8749, 8750, 8751, 8752, 8753, 8754, 8755, 8756, 8765, 8767, 8769, 8820, 8822, 8824, 8826, 8859, 8872, 8874, 8877, 8880, 8925, 8951, 8961, 8965, 8966, 8967, 8968, 8969, 8970, 8971, 8972, 8974, 8975, 8976, 8977, 8978, 8979, 8982, 8983, 8986, 8989, 8996, 9006, 9023, 9048, 9060, 9062, 9064, 9077, 9082, 9086, 9090, 9108, 9145, 9173, 9178, 9180, 9188, 9194, 9196, 9197, 9219, 9244, 9245, 9249, 9251, 9263, 9264, 9265, 9268, 9271, 9272, 9273, 9274, 9280, 9285, 9296, 9312, 9440, 9441, 9445, 9447, 9448, 9450, 9451, 9471, 9492, 9499, 9503, 9681, 9684, 9685, 9687, 9688, 9689, 9690, 9691, 9693, 9694, 9696, 9697, 9699, 9700, 9701, 9702, 9703, 9704, 9706, 9709, 9710, 9712, 9713, 9715, 9716, 9717, 9718, 9720, 9721, 9722, 9723, 9724, 9726, 9739, 9741, 9743, 9746, 9748, 9749, 9751, 9752, 9756, 9813, 9839, 9842, 9846, 9848, 9850, 9851, 9854, 9855, 9858, 9861, 9879, 9881, 9882, 9884, 9885, 9888, 9896, 9902, 9905, 9906, 9907, 9981, 9984, 9985, 9986, 9987, 10008, 10009, 10013, 10016, 10034, 10040, 10045, 10058, 10094, 10095, 10096, 10098, 10099, 10100, 10101, 10102, 10104, 10138, 10780, 10781, 10782, 10784, 10797, 10798, 10800, 10801, 10805, 10807, 10811, 10816, 10817, 10818, 10819, 10822, 10825, 10827, 10830, 10833, 10836, 10838, 10842, 10919, 10920, 10921, 10922, 10923, 10924, 10925, 10926, 10927, 10929, 10934, 10964, 10987, 10988, 10990, 10994, 10996, 11027, 11030, 11036, 11037, 11038, 11041, 11043, 11052, 11055, 11057, 11061, 11063, 11511, 11518, 11612, 11614, 11620, 11628, 11630, 11634, 11636, 11637, 11639, 11641, 11642, 11643, 11644, 11645, 11646, 11647, 11648, 11650, 11652, 11653, 11654, 11655, 11656, 11658, 11660, 11676, 11687, 11693, 11695, 11697, 11699, 11700, 11701, 11702, 11704, 11706, 11708, 11709, 11733, 11734, 11735, 11736, 11740, 11741, 11754, 11755, 11756, 11757, 11758, 11759, 11760, 11765, 11780, 11782, 11783, 11784, 11785, 11787, 11789, 11790, 11791, 11792, 11793, 11794, 11820, 11821, 11822, 11823, 11824, 11826, 11827, 11831, 11833, 11836, 11838, 11839, 11840, 11842, 11844, 11851, 11854, 11858, 11859, 11860, 11869, 11871, 11872, 11873, 11876, 11877, 11878, 11879, 11881, 11882, 11884, 11886, 11888, 11894, 11896, 11916, 11918, 11921, 11925, 11926, 11930, 11942, 11957, 12012, 12013, 12017, 12023, 12027, 12040, 12042, 12045, 12047, 12048, 12169, 12176, 12180, 12255, 12256, 12261, 12262, 12297, 12300, 12302, 12304, 12322, 12323, 12328, 12331, 12335, 12340, 12347, 12349, 12353, 12355, 12358, 12366, 12372, 12377, 12388, 12391, 12394, 12395, 12399, 12404, 12405, 12415, 12417, 12421, 12424, 12425, 12426, 12429, 12432, 12436, 12442, 12444, 12447, 12462, 12473, 12478, 12479, 12483, 12507, 12508, 12517, 12520, 12525, 12526, 12531, 12533, 12536, 12537, 12538, 12540, 12541, 12543, 12550, 12555, 12560, 12566, 12567, 12570, 12571, 12572, 12578, 12579, 12581, 12600, 12603, 12604, 12609, 12610, 12611, 12618, 12619, 12627, 12631, 12636, 12638, 12640, 12642, 12644, 12646, 12647, 12648, 12649, 12652, 12653, 12656, 12657, 12663, 12666, 12667, 12669, 12678, 12679, 12680, 12681, 12682, 12687, 12688, 12691, 12699, 12714, 12715, 12722, 12726, 12727, 12728, 12738, 12741, 12747, 12752, 12754, 12755, 12758, 12775, 12783, 12784, 12799, 12804, 12808, 12816, 12821, 12831, 12832, 12844, 12845, 12856, 12858, 12859, 12861, 12866, 12870, 12872, 12879, 12880, 12882, 12884, 12888, 12894, 12896, 12897, 12899, 12900, 12917, 12918, 12921, 12927, 12928, 12930, 12932, 12933, 12936, 12937, 12940, 12948, 12953, 12955, 12962, 12963, 12964, 12973, 12984, 12993, 13015, 13016, 13021, 13023, 13035, 13049, 13051, 13058, 13060, 13064, 13076, 13077, 13109, 13111, 13128, 13139, 13143, 13150, 13151, 13155, 13164, 13172, 13174, 13179, 13182, 13189, 13197, 13200, 13205, 13208, 13212, 13213, 13220, 13226, 13232, 13239, 13251, 13264, 13266, 13267, 13271, 13279, 13280, 13283, 13303, 13304, 13306, 13315, 13324, 13326, 13332, 13334, 13336, 13340, 13342, 13345, 13361, 13369, 13370, 13393, 13402, 13408, 13412, 13419, 13435, 13455, 13458, 13464, 13465, 13471, 13472, 13475, 13491, 13506, 13507, 13516, 13518, 13529, 13537, 13545, 13546, 13562, 13563, 13575, 13578, 13579, 13585, 13589, 13592, 13599, 13602, 13607, 13616, 13620, 13631, 13633, 13638, 13648, 13670, 13676, 13680, 13684, 13699, 13704, 13712, 13716, 13727, 13730, 13734, 13738, 13747, 13762, 13769, 13801, 13833, 13836, 13838, 13847, 13848, 13852, 13854, 13856, 13859, 13862, 13863, 13864, 13938, 13943, 13944, 13945, 13946, 13948, 13949, 13950, 13952, 13953, 13957, 13959, 13960, 13963, 13966, 13971, 13972, 13973, 14111, 14112, 14117, 14122, 14124, 14132, 14142, 14160, 14161, 14164, 14165, 14167, 14170, 14185, 14192, 14195, 14198, 14199, 14200, 14201, 14203, 14227, 14229, 14230, 14232, 14234, 14237, 14238, 14268, 14270, 14271, 14278, 14285, 14298, 14304, 14318, 14320, 14321, 14331, 14334, 14335, 14339, 14340, 14363, 14364, 14366, 14400, 14403, 14406, 14416, 14420, 14421, 14423, 14424, 14425, 14426, 14428, 14446, 14447, 14450, 14451, 14452, 14453, 14454, 14455, 14457, 14458, 14459, 14463, 14464, 14466, 14467, 14468, 14486, 14488, 14493, 14496, 14497, 14498, 14500, 14501, 14502, 14503, 14504, 14507, 14510, 14512, 14513, 14514, 14515, 14518, 14520, 14521, 14522, 14523, 14535, 14536, 14539, 14544, 14548, 14550, 14551, 14552, 14554, 14555, 14558, 14578, 14580, 14582, 14583, 14588, 14590, 14607, 14609, 14613, 14615, 14617, 14621, 14636, 14637, 14638, 14639, 14691, 14694, 14697, 14698, 14699, 14700, 14708, 14709, 14714, 14719, 14720, 14721, 14725, 14727, 14731, 14733, 14734, 14736, 14777, 14818, 14819, 14822, 14828, 14831, 14833, 14834, 14836, 14837, 14838, 14849, 14850, 14852, 14856, 14857, 14860, 14863, 14886, 14888, 14889, 14890, 14891, 14893, 14894, 14907, 14908, 14910, 14914, 14917, 14919, 14926, 14931, 14932, 14934, 14942, 14944, 14946, 14949, 14954, 14955, 14956, 14960, 14961, 14962, 14996, 14997, 14999, 15000, 15001, 15004, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15015, 15016, 15018, 15019, 15020, 15021, 15022, 15023, 15025, 15026, 15027, 15030, 15031, 15032, 15033, 15034, 15035, 15037, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15065, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15077, 15078, 15079, 15080, 15105, 15109, 15114, 15115, 15117, 15122, 15128, 15132, 15134, 15135, 15136, 15138, 15140, 15141, 15143, 15224, 15225, 15226, 15227, 15229, 15234, 15236, 15237, 15238, 15239, 15240, 15243, 15244, 15246, 15247, 15250, 15251, 15252, 15254, 15256, 15259, 15262, 15264, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15294, 15295, 15312, 15313, 15331, 15333, 15335, 15336, 15339, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15352, 15354, 15355, 15357, 15359, 15360, 15362, 15364, 15365, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15378, 15382, 15422, 15427, 15444, 15464, 15588, 15601, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15642, 15650, 15651, 15652, 15657, 15667, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15677, 15678, 15679, 15680, 15681, 15682, 15684, 15685, 15686, 15687, 15689, 15690, 15691, 15782, 15783, 15785, 15786, 15791, 15834, 15862, 15875, 15878, 15883, 15908, 15926, 15937, 15965, 15966, 15968, 15970, 15974, 15976, 15978, 15979, 15980, 15983, 15984, 15985, 15986, 15987, 15989, 15991, 15993, 15995, 15996, 15997, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16008, 16010, 16013, 16026, 16028, 16034, 16035, 16036, 16040, 16043, 16044, 16045, 16047, 16048, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16085, 16086, 16089, 16091, 16093, 16095, 16128, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16153, 16160, 16188, 16190, 16191, 16194, 16195, 16304, 16305, 16356, 16367, 16399, 16424, 16425, 16437, 16467, 16499, 16536, 16543, 16544, 16576, 16581, 16615, 16622, 16624, 16627, 16634, 16641, 16648, 16649, 16660, 16673, 16676, 16677, 16678, 16681, 16719, 16735, 16740, 16751, 16752, 16753, 16754, 16755, 16757, 16773, 16838, 16841, 16844, 16846, 16847, 16849, 16858, 16870, 16878, 16879, 16882, 16883, 16887, 16893, 16894, 16897, 16903, 16904, 16909, 16911, 16933, 16941, 16944, 16946, 16950, 16967, 16969, 16970, 16976, 16996, 16998, 17014, 17018, 17032, 17033, 17034, 17037, 17080, 17084, 17085, 17090, 17093, 17098, 17100, 17102, 17105, 17113, 17120, 17128, 17131, 17185, 17200, 17211, 17222, 17223, 17231, 17272, 17281, 17285, 17294, 17297, 17314, 17321, 17328, 17334, 17335, 17336, 17337, 17342, 17378, 17384, 17389, 17392, 17394, 17395, 17397, 17423, 17425, 17433, 17437, 17450, 17474, 17477, 17501, 17504, 17535, 17540, 17543, 17548, 17549, 17562, 17575, 17578, 17582, 17584, 17590, 17591, 17594, 17606, 17613, 17626, 17649, 17656, 17659, 17660, 17686, 17694, 17696, 17711, 17735, 17739, 17742, 17762, 17763, 17767, 17794, 17796, 17802, 17807, 17812, 17816, 17817, 17830, 17853, 17854, 17867, 17878, 17880, 17881, 17897, 17899, 17910, 17911, 17932, 17941, 17945, 17959, 17973, 17977, 17981, 17982, 17985, 18018, 18022, 18024, 18026, 18034, 18041, 18045, 18051, 18057, 18072, 18075, 18082, 18087, 18090, 18091, 18097, 18108, 18112, 18113, 18124, 18132, 18144, 18162, 18178, 18181, 18185, 18188, 18208, 18213, 18237, 18268, 18276, 18285, 18287, 18290, 18291, 18300, 18301, 18312, 18318, 18320, 18321, 18332, 18335, 18336, 18339, 18364, 18367, 18369, 18372, 18381, 18385, 18411, 18417, 18422, 18445, 18451, 18452, 18458, 18470, 18478, 18491, 18492, 18508, 18511, 18512, 18518, 18521, 18525, 18530, 18531, 18537, 18540, 18543, 18576, 18582, 18586, 18588, 18591, 18604, 18611, 18629, 18630, 18637, 18683, 18716, 18785, 18790, 18858, 18898, 18899, 18906, 18908, 18914, 18915, 19007, 19009, 19062, 19065, 19068, 19072, 19073, 19074, 19076, 19109, 19115, 19122, 19123, 19132, 19133, 19137, 19140, 19142, 19144, 19145, 19200, 19211, 19214, 19215, 19217, 19218, 19224, 19226, 19227, 19228, 19229, 19231, 19236, 19237, 19244, 19249, 19252, 19261, 19267, 19274, 19277, 19300, 19301, 19314, 19315, 19317, 19318, 19322, 19352, 19354, 19356, 19364, 19381, 19432, 19436, 19440, 19444, 19476, 19479, 19481, 19485, 19486, 19487, 19493, 19496, 19519, 19520, 19523, 19524, 19527, 19528, 19529, 19550, 19557, 19560, 19587, 19588, 19589, 19590, 19591, 19592, 19594, 19595, 19758, 19772, 19774, 19775, 19777, 19781, 19784, 19787, 19789, 19793, 19804, 19805, 19808, 19809, 19810, 19821, 19878, 19897, 19953, 19954, 19984, 19986, 19987, 19988, 19989, 19990, 19992, 19994, 19995, 19999, 20091, 20094, 20095, 20122, 20125, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20136, 20137, 20139, 20141, 20207, 20210, 20213, 20236, 20282, 20318, 20399, 20403, 20417, 20418, 20419, 20430, 20448, 20493, 20494, 20498, 20499, 20510, 20535, 20544, 20554, 20558, 20569, 20573, 20575, 20576, 20577, 20578, 20591, 20596, 20598, 20602, 20620, 20622, 20627, 20635, 20644, 20645, 20646, 20654, 20664, 20699, 20700, 20701, 20705, 20710, 20711, 20748, 20780, 20781, 20786, 20788, 20798, 20799, 20801, 20808, 20812, 20820, 20821, 20828, 20841, 20842, 20847, 20852, 20862, 20869, 20872, 20873, 20879, 20885, 20886, 20891, 20892, 20896, 20899, 20900, 20901, 20904, 20905, 20912, 20929, 20930, 20945, 20946, 20949, 20952, 20992, 21000, 21001, 21006, 21014, 21016, 21020, 21022, 21046, 21068, 21093, 21098, 21101, 21140, 21165, 21191, 21194, 21196, 21201, 21204, 21229, 21231, 21232, 21234, 21264, 21276, 21280, 21402, 21430, 21445, 21447, 21451, 21458, 21463, 21494, 21500, 21535, 21543, 21544, 21553, 21554, 21571, 21572, 21575, 21577, 21591, 21592, 21593, 21594, 21595, 21597, 21598, 21599, 21602, 21612, 21613, 21628, 21637, 21644, 21645, 21710, 21747, 21755, 21758, 21760, 21774, 21777, 21778, 21779, 21781, 21789, 21790, 21796, 21799, 21803, 21814, 21828, 21830, 21833, 21841, 21858, 21859, 21861, 21864, 21869, 21873, 21875, 21890, 21899, 21900, 21902, 21909, 21910, 21911, 21914, 21915, 21916, 21919, 21930, 21939, 21940, 21944, 21954, 21960, 21967, 21972, 21977, 21981, 21987, 21990, 22002, 22011, 22028, 22033, 22041, 22053, 22065, 22083, 22091, 22094, 22096, 22110, 22112, 22139, 22161, 22162, 22181, 22195, 22198]\n"
     ]
    }
   ],
   "source": [
    "DATASETS_DIR = r\"../datasets\"\n",
    "ADE_DIR = os.path.join(DATASETS_DIR, r\"ADE\")\n",
    "\n",
    "def separate_ade_indoor_outdoor():\n",
    "    df_ade = load_dataframe(filename=\"ADE_master.csv\", dataset_dir=ADE_DIR, header=0, index_col=None)\n",
    "    print(df_ade.shape[0])\n",
    "\n",
    "\n",
    "    # Indoor\n",
    "    has_wall = df_ade['wall'] > 0.0\n",
    "    print(has_wall.sum())\n",
    "\n",
    "    has_floor = df_ade['floor'] > 0.0\n",
    "    print(has_floor.sum())\n",
    "\n",
    "    has_wall_or_floor = np.logical_or(has_wall, has_floor)\n",
    "    print(has_wall_or_floor.sum())\n",
    "\n",
    "    wall_more_sky = df_ade['wall'] > df_ade['sky']\n",
    "    print(wall_more_sky.sum())\n",
    "\n",
    "    # Either is a picture of house with floor or if no floor, there would be walls (wall > sky means to include possible windows)\n",
    "    indoor = np.logical_or(wall_more_sky, has_floor)\n",
    "    print(\"Indoor: {}\".format(indoor.sum()))\n",
    "\n",
    "    x_indoor = np.logical_xor(indoor, wall_more_sky)\n",
    "    print(\"XIndoor: {}\".format(x_indoor.sum()))\n",
    "    print(df_ade['id'][x_indoor])\n",
    "\n",
    "\n",
    "    # Outdoor\n",
    "    has_sky = df_ade['sky'] > 0.0\n",
    "    print(has_sky.sum())\n",
    "\n",
    "    has_road = df_ade['road'] > 0.0\n",
    "    print(has_road.sum())\n",
    "\n",
    "    has_sidewalk = df_ade['sidewalk'] > 0.0\n",
    "    print(\"Sidewalk: {}\".format(has_sidewalk.sum()))\n",
    "\n",
    "    has_road_no_sidewalk = np.logical_and(has_road, ~has_sidewalk)\n",
    "    print(\"Road no sidewalk: {}\".format(has_road_no_sidewalk.sum()))\n",
    "    #print(df_ade['id'][has_road_no_sidewalk])\n",
    "\n",
    "\n",
    "    # Isolate possible sidewalk/road environments using cars/trucks/vans as reference\n",
    "    has_car = df_ade['car'] > 0.0\n",
    "    has_truck = df_ade['truck'] > 0.0\n",
    "    has_van = df_ade['van'] > 0.0\n",
    "\n",
    "    has_vehicle = np.logical_or(np.logical_or(has_car, has_truck), has_van)\n",
    "    has_road_no_vehicle = np.logical_and(has_road, ~has_vehicle)\n",
    "\n",
    "    print(has_road_no_vehicle.sum())\n",
    "\n",
    "    has_grass = df_ade['grass'] > 0.0\n",
    "    has_grass_no_indoor = np.logical_and(has_grass, ~indoor)\n",
    "    print(\"Walkable grass: {}\".format(has_grass_no_indoor.sum()))\n",
    "\n",
    "    print(list(df_ade['id'][has_grass_no_indoor]))\n",
    "    \n",
    "    \n",
    "def run_separate_ade_indoor_outdoor():\n",
    "    separate_ade_indoor_outdoor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b4c9a",
   "metadata": {},
   "source": [
    "## Decide whether to interpolate based on the missing on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c19d134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00    0.000000\n",
      "0.05    0.007164\n",
      "0.25    0.014094\n",
      "0.50    0.027032\n",
      "0.75    0.091356\n",
      "0.90    0.269002\n",
      "0.95    0.423342\n",
      "1.00    1.000000\n",
      "Name: invalid, dtype: float64\n",
      "0.00    0.000000\n",
      "0.05    0.000000\n",
      "0.25    0.000000\n",
      "0.50    0.000000\n",
      "0.75    0.009342\n",
      "0.90    0.074192\n",
      "0.95    0.212617\n",
      "1.00    0.958399\n",
      "Name: invalid, dtype: float64\n",
      "0.00    0.000000\n",
      "0.05    0.000000\n",
      "0.10    0.000000\n",
      "0.15    0.000000\n",
      "0.20    0.000616\n",
      "0.25    0.006334\n",
      "0.30    0.009039\n",
      "0.35    0.011261\n",
      "0.40    0.013458\n",
      "0.45    0.015784\n",
      "0.50    0.018402\n",
      "0.55    0.021820\n",
      "0.60    0.026721\n",
      "0.65    0.033970\n",
      "0.70    0.045306\n",
      "0.75    0.062729\n",
      "0.80    0.090853\n",
      "0.85    0.137828\n",
      "0.90    0.223347\n",
      "0.95    0.379932\n",
      "1.00    1.000000\n",
      "Name: invalid, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Do not interpolate erroneous data that has too much invalid labels \\n# Erroneous data 1: images with large amount of invalid labels\\n# Here we will cull 2/10 (20%) of the data with the most invalid nodes\\nqt, bins = pd.qcut(df_ade[\\'invalid\\'], q=10, precision=0, retbins=True)\\nprint(qt)\\nthreshold_invalid_ratio = bins[8]\\nprint(\"Threshold invalid: {}\".format(threshold_invalid_ratio))\\n#ADE_df_cleaned = ADE_df[ADE_df[\\'invalid\\'] <= threshold_invalid_ratio]\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS_DIR = r\"../datasets\"\n",
    "PASCAL2010_DIR = os.path.join(DATASETS_DIR, r\"PASCAL2010\")\n",
    "ADE_DIR = os.path.join(DATASETS_DIR, r\"ADE\")\n",
    "\n",
    "\n",
    "def print_missing_data_quantiles():\n",
    "    df_ade = load_dataframe(filename=\"ADE_master.csv\", dataset_dir=ADE_DIR, header=0, index_col=None)\n",
    "    df_pascal2010 = load_dataframe(filename=\"PASCAL2010_master.csv\", dataset_dir=PASCAL2010_DIR, header=0, index_col=None)\n",
    "\n",
    "\n",
    "    ade_invalid_quantiles = df_ade['invalid'].quantile([0, 0.05, 0.25,0.5,0.75, 0.90, 0.95, 1.0])\n",
    "    print(ade_invalid_quantiles)\n",
    "\n",
    "    pascal2010_invalid_quantiles = df_pascal2010['invalid'].quantile([0, 0.05, 0.25,0.5,0.75, 0.90, 0.95, 1.0])\n",
    "    print(pascal2010_invalid_quantiles)\n",
    "\n",
    "\n",
    "    df_master = pd.concat([df_ade, df_pascal2010])\n",
    "    master_invalid_quantiles = df_master['invalid'].quantile([i*0.01 for i in range(0, 100+5, 5)])\n",
    "    print(master_invalid_quantiles)\n",
    "    \n",
    "def run_print_missing_data_quantiles():\n",
    "    print_missing_data_quantiles()\n",
    "\n",
    "'''\n",
    "# Do not interpolate erroneous data that has too much invalid labels \n",
    "# Erroneous data 1: images with large amount of invalid labels\n",
    "# Here we will cull 2/10 (20%) of the data with the most invalid nodes\n",
    "qt, bins = pd.qcut(df_ade['invalid'], q=10, precision=0, retbins=True)\n",
    "print(qt)\n",
    "threshold_invalid_ratio = bins[8]\n",
    "print(\"Threshold invalid: {}\".format(threshold_invalid_ratio))\n",
    "#ADE_df_cleaned = ADE_df[ADE_df['invalid'] <= threshold_invalid_ratio]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b788f",
   "metadata": {},
   "source": [
    "## Create the first train_split_test via  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6da7f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  id dataset  width  height             scene   invalid\n",
      "0      0   0     ADE    683     512  airport_terminal  0.098448\n",
      "1      1   1     ADE    711     512  airport_terminal  0.036494\n",
      "2      2   2     ADE    683     512       art_gallery  0.200949\n",
      "3      3   3     ADE    384     251          badlands  0.006111\n",
      "4      4   4     ADE    683     512          ball_pit  0.733860\n",
      "0        0.098448\n",
      "1        0.036494\n",
      "2        0.200949\n",
      "3        0.006111\n",
      "4        0.733860\n",
      "           ...   \n",
      "22205    0.005130\n",
      "22206    0.023865\n",
      "22207    0.012251\n",
      "22208    0.420563\n",
      "22209    0.706917\n",
      "Name: invalid, Length: 22210, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# combine only by invalid cases, so we can train_test_split and then find out how muhc to interpolate based ontrain\n",
    "DATASETS_DIR = r\"../datasets\"\n",
    "PASCAL2010_DIR = os.path.join(DATASETS_DIR, r\"PASCAL2010\")\n",
    "ADE_DIR = os.path.join(DATASETS_DIR, r\"ADE\")\n",
    "\n",
    "\n",
    "df_ade = load_dataframe(filename=\"ADE_master.csv\", dataset_dir=ADE_DIR, header=0, index_col=None)\n",
    "df_pascal2010 = load_dataframe(filename=\"PASCAL2010_master.csv\", dataset_dir=PASCAL2010_DIR, header=0, index_col=None)\n",
    "\n",
    "\n",
    "df_ade = df_ade.iloc[:,:7]\n",
    "print(df_ade.head())\n",
    "\n",
    "ade_invalid = df_ade['invalid']\n",
    "print(ade_invalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and Separate into two datasets: One for inside and one for outside\n",
    "#     This is based on correlation between scene and wall; ceiling; sky [floor, wall, ceiling, sky]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b8a49",
   "metadata": {},
   "source": [
    "## Decide whether to interpolate based on the missing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9592c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cab89ba",
   "metadata": {},
   "source": [
    "## Combine the ADE and PASCAL image dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "583ddffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_JPEG_IMAGES_DIR = r\"G:\\segmentation_tinker\\datasets\\ADEChallengeData2016\\images\"\n",
    "COMMON_JPEG_IMAGES_DIR = r\"G:\\segmentation_environment\\datasets\\ADE_PASCAL2010\\images\"\n",
    "OFFSET = 22210\n",
    "Z_FILL=8\n",
    "\n",
    "# ADE INDEX IS 0-22209\n",
    "# PASCAL INDEX 22210-33530\n",
    "\n",
    "def rename_pascal(images_dir=PASCAL_2010_JPEG_IMAGES_DIR, common_images_dir=COMMON_JPEG_IMAGES_DIR, offset=OFFSET):\n",
    "    for i, image_file in enumerate(os.listdir(images_dir)):\n",
    "        deconstructed_file_name = re.split('\\_|\\.', image_file)\n",
    "        new_image_file = \"IMAGE\" + \"_\" + str(offset+i).zfill(Z_FILL) + \".\" + deconstructed_file_name[2]\n",
    "        shutil.copy(os.path.join(images_dir, image_file), os.path.join(common_images_dir, new_image_file))\n",
    "\n",
    "        \n",
    "def rename_ade(images_dir=ADE_JPEG_IMAGES_DIR, common_images_dir=COMMON_JPEG_IMAGES_DIR, offset=OFFSET):\n",
    "    for i, image_file in enumerate(os.listdir(images_dir)):\n",
    "        deconstructed_file_name = re.split('\\_|\\.', image_file)\n",
    "        new_image_file = \"IMAGE\" + \"_\" + str(int(deconstructed_file_name[2])).zfill(Z_FILL) + \".\" + deconstructed_file_name[3]\n",
    "        shutil.copy(os.path.join(images_dir, image_file), os.path.join(common_images_dir, new_image_file))\n",
    "\n",
    "    \n",
    "def run_combine_ade_pascal_images():\n",
    "    rename_pascal()\n",
    "    rename_ade()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618cdaf",
   "metadata": {},
   "source": [
    "## Combine the ADE and PASCAL labels dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a94398cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Note, the labels in the common labels directory hold two kinds of labels: \n",
    "one for ADE which has a class range of [0,150] and the other for PASCAL2010 which has a class range of [0, 459].\n",
    "It is up to the labels in the methodology to be cleaned, preprocessed, and consistent.\n",
    "The common labels directory should just be used as a baseline reference.\n",
    "'''\n",
    "\n",
    "COMMON_LABELS_DIR = r\"G:\\segmentation_environment\\datasets\\ADE_PASCAL2010\\labels\"\n",
    "PASCAL_LABELS_DIR = os.path.join(PASCAL_2010_DIR, r\"SegmentationRawClass\")\n",
    "\n",
    "\n",
    "def rename_ade_labels(labels_dir=COMMON_LABELS_DIR):\n",
    "    for label_file in os.listdir(COMMON_LABELS_DIR):\n",
    "        deconstructed_file_name = re.split('\\_|\\.', label_file)\n",
    "        if deconstructed_file_name[0] == \"ADE\":\n",
    "            new_file_name = \"LABEL\" + \"_\" + str(int(deconstructed_file_name[-2])).zfill(Z_FILL) + \".\" + deconstructed_file_name[-1]\n",
    "            os.rename(os.path.join(labels_dir, label_file), os.path.join(labels_dir, new_file_name))\n",
    "\n",
    "\n",
    "def concatenate_pascal_labels(labels_dir=COMMON_LABELS_DIR, pascal_labels_dir=PASCAL_LABELS_DIR, offset=OFFSET):\n",
    "    for i, matrix_file in enumerate(os.listdir(pascal_labels_dir)):\n",
    "        mat_label = io.loadmat(os.path.join(pascal_labels_dir, matrix_file))['LabelMap']\n",
    "        label_file_name = \"LABEL\" + \"_\" + str(i+offset).zfill(Z_FILL) + \".\" + \"npy\"\n",
    "        np.save(os.path.join(labels_dir, label_file_name), mat_label)\n",
    "\n",
    "    \n",
    "def run_combine_ade_pascal_labels():\n",
    "    rename_ade_labels()\n",
    "    concatenate_pascal_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16670edb",
   "metadata": {},
   "source": [
    "## Find any amount of invalid labels in ADE_PASCAL2010 to be candidates for interpolation or culling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb5f3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Series([], Name: id, dtype: int64)\n",
      "0\n",
      "Series([], Name: id, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "PASCAL2010_CSV = r\"G:\\segmentation_master\\datasets\\PASCAL2010\\dataframes\\PASCAL2010_master.csv\"\n",
    "ADE_CSV = r\"G:\\segmentation_master\\datasets\\ADE\\dataframes\\ADE_master.csv\"\n",
    "\n",
    "def find_invalid_set(csv):\n",
    "    df = pd.read_csv(csv, header=0, sep=',', lineterminator='\\n')\n",
    "    print((df['invalid'] != 0).sum())\n",
    "    print(df[df['invalid'] != 0]['id'])\n",
    "    \n",
    "    \n",
    "def run_find_invalid():\n",
    "    invalid_set_ade = find_invalid_set(csv=ADE_CSV)\n",
    "    invalid_set_pascal = find_invalid_set(csv=PASCAL2010_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3215b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc483_tf1]",
   "language": "python",
   "name": "conda-env-cpsc483_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
